**CARE: A Concurrency‑Aware Enhanced Lightweight Cache Management Framework**
**Xiaoyang Lu**
Department of Computer Science
Illinois Institute of Technology
Chicago, Illinois
[xlu40@hawk.iit.edu](mailto:xlu40@hawk.iit.edu)

**Rujia Wang**
Department of Computer Science
Illinois Institute of Technology
Chicago, Illinois
[rwang67@iit.edu](mailto:rwang67@iit.edu)

**Xian‑He Sun**
Department of Computer Science
Illinois Institute of Technology
Chicago, Illinois
[sun@iit.edu](mailto:sun@iit.edu)

---

### Abstract

Improving cache performance is a lasting research topic. While utilizing data locality to enhance cache performance becomes more and more difficult, data access concurrency provides a new opportunity for cache performance optimization. In this work, we propose a novel concurrency‑aware cache management framework that outperforms state‑of‑the‑art locality‑only cache management schemes. First, we investigate the merit of data access concurrency and pinpoint that reducing the miss rate may not necessarily lead to better overall performance. Next, we introduce the pure miss contribution (PMC) metric, a lightweight and versatile concurrency‑aware indicator, to accurately measure the cost of each outstanding miss access by considering data concurrency. Then, we present CARE, a dynamic adjustable, concurrency‑aware, low‑overhead cache management framework with the help of the PMC metric. We evaluate CARE with extensive experiments across different application domains and show significant performance gains with the consideration of data concurrency. In a 4‑core system, CARE improves IPC by 10.3% over LRU replacement. In 8‑ and 16‑core systems where more concurrent data accesses exist, CARE outperforms LRU by 13.0% and 17.1%, respectively.

---

## I. INTRODUCTION

Intensive research has been conducted to address the memory wall problem [50], of which improving locality and concurrency are two fundamental approaches. Cache hierarchies utilize data locality to minimize the long delay of off‑chip main memory accesses. Significant research focuses on taking advantage of data locality, resulting in many schemes that detect memory access patterns, so that cache eviction and insertion decisions can be determined by the reference predictions to reduce cache miss rate [8], [11], [13], [14], [15], [17], [18], [19], [20], [21], [23], [25], [26], [27], [33], [35], [36], [38], [39], [41], [45], [47], [48], [51], [53]. Although such locality‑based cache management frameworks may reduce the number of misses, we find that considering both locality and concurrency can further improve the state‑of‑the‑art locality‑only optimizations.

Modern high‑performance processors support data access concurrency [12] with advanced caching techniques such as multi‑port, multi‑bank, pipelined, and non‑blocking cache. As a result, multiple outstanding cache accesses can be generated by one processor and overlapped with each other. With data access concurrency in the memory hierarchy, the cost of a miss could vary. Some misses are isolated, some misses occur concurrently with other hits, and some misses overlap with other miss accesses [29]. The performance loss resulting from a cache miss can be hidden by access overlapping. Thus, a more accurate cost metric for cache misses may help improve cache performance further when data concurrency and overlapping exist [30], [34], [44], [52].

In this work, we first introduce and formally define the concept of **Pure Miss Contribution (PMC)**. PMC is a new cost metric for cache misses, with a comprehensive analysis of both hit‑miss and miss‑miss overlapping in the memory system. PMC has high predictability and versatility. We observe that the PMC values of the misses caused by the same program counter (PC) are relatively stable; therefore, the past PMC value can be used to predict the future PMC value of the same load instruction. PMC is also lightweight to measure and versatile enough to be used to build concurrency‑aware cache management frameworks. We then present **CARE**, a concurrency‑aware cache management framework that takes both data locality and concurrency into account. CARE learns the re‑reference behavior and PMC value of each miss access to guide future replacement decisions. CARE augments existing cache insertion and hit‑promotion policies to reserve a small subset of performance‑critical blocks with high locality and high PMC, and evict dead blocks or blocks with low PMC. CARE is also prefetch‑aware, and it performs well under prefetchers. In CARE, we also implement a **Dynamic Threshold Reconfiguration Mechanism (DTRM)**, which enables CARE to better adapt to different applications and execution phases. Our experimental results show that CARE outperforms state‑of‑the‑art cache management schemes. Furthermore, CARE has low overhead and can be practically implemented in hardware. To summarize, this paper makes the following contributions:

1. We introduce the pure miss contribution (PMC), a novel and accurate metric to quantify the cost and performance impact of outstanding cache misses. We describe how PMC can be measured in modern cache hierarchies. We find that PMC is predictable and can be used for cache optimization.

2. We present CARE, a comprehensive cache management framework that considers both locality and concurrency. CARE is general for all types of applications, practical with low hardware implementation overhead, and adaptive with a novel Dynamic Threshold Reconfiguration Mechanism (DTRM).

3. Our evaluations show that CARE substantially improves upon existing state‑of‑the‑art cache management schemes over a wide variety of workloads in a wide range of system configurations and performs well with data prefetching.

---

## II. BACKGROUND AND PRELIMINARIES

### A. Memory Level Parallelism

Multi‑core and multi‑threading designs, as well as advanced caching techniques [7], [13], [24], [34], [37], increase data access concurrency. As a result, a number of memory accesses can concurrently coexist in the memory hierarchy. In this case, some memory accesses may overlap with others, which reduces their performance impact on cores.

**Memory Level Parallelism (MLP)** can be used to measure miss concurrency. MLP captures the number of outstanding cache misses that can be generated and executed in an overlapped manner [16]. Some misses are isolated, while some occur concurrently with other misses. The more cache misses occur concurrently, the smaller the impact of each cache miss on performance since all concurrent misses will amortize the total memory stall cycles. Therefore, based on the MLP concept, isolated misses are considered to hurt performance more than concurrent misses. MLP can be measured with **MLP‑based cost** [34]. The MLP‑based cost of an isolated miss can be approximated by the number of miss‑access cycles that the miss spends. For concurrent misses, the data access delay is divided equally among all concurrent outstanding misses, representing the MLP‑based cost of each concurrent miss access.

MLP‑based cost can identify costly misses by considering the miss‑miss overlapping. However, we find that hit‑miss overlapping impacts the cost of misses as well, and modern memory systems have a lot of such overlapping accesses (details in Section III‑B). Therefore, we need a holistic metric that is able to catch all types of overlapping and provide a better memory performance optimization guidance.

### B. Concurrent Memory Access Model

To capture all types of concurrent memory accesses and quantify their impact on performance, a concurrent memory access model named **C‑AMAT** was proposed [44]. In the C‑AMAT performance model, a cache access latency is composed of two parts:

1. **base access cycles**, which are the minimum time an access (hit or miss) needs to spend on a specific cache level;
2. **miss access cycles**, which are the additional time spent waiting for data in the next levels of the memory hierarchy.

For a miss access, the tag lookup time is considered to be the base access cycles of the access. A miss access latency consists of both base access cycles and miss access cycles.

Based on the C‑AMAT model, the miss access cycles can be hidden when there is a hit‑miss overlapping. It refers to miss access cycles overlapped with the base access cycles of a hit/miss access. The actual cost of the misses should be revisited. On the other side, for a miss access cycle that does not overlap with any base access cycle, we refer to this cycle as a **pure miss cycle** [28], [29], [30], [31]. If a miss access contains at least one pure miss cycle, this miss is categorized as **pure miss**. Pure miss has a higher performance impact because pure miss cycles have no overlapping base access cycles to hide the penalty. Similar to miss rate, the **Pure Miss Rate (pMR)** can measure the cache efficiency by considering data access concurrency:

[
\text{Pure Miss Rate (pMR)} = \frac{\text{Num. of Pure Misses}}{\text{Num. of Total Accesses}}
]

Based on the C‑AMAT model, the **memory active cycles** on a memory layer are the cycles with memory activities [28]. Active miss cycles are classified into two categories: **active pure miss cycles** and **active non‑pure miss cycles**. Active pure miss cycles are the cycles that only contain the pure miss cycles, and these cycles cause more performance degradation. The active non‑pure miss cycles do not introduce heavy degradation, as the miss access cycle is overlapped and hidden.

The C‑AMAT model is general and can be applied to each level of the memory hierarchy. In multi‑core systems, the model works by tracking the overlapping from each core. In other words, the pure miss in a multi‑core system contains at least one miss access cycle without any overlapped base access cycles **from the same core**. We find that the concepts in C‑AMAT can capture all types of memory access overlapping. If we can quantify the cost of memory misses with all types of overlapping, we can use the metric to enable cache optimization further. In this work, we present the **pure miss contribution** metric (details in Section IV), which is inspired by the C‑AMAT model, and it shows great potential to be incorporated with cache optimization frameworks.

### C. Locality‑based Cache Management

Locality‑based cache management schemes are designed to increase performance by reducing the total number of misses. The ideal upper bound for such schemes is Belady’s optimal replacement (OPT) [10], which always evicts the block with the largest future usage distance. Recent locality‑based cache management studies have focused on exploring prediction‑based schemes to reduce the number of cache misses [17], [19], [40], [42], [45], [48].

**Re‑reference prediction.** Several replacement studies are designed based on the re‑reference prediction of cache blocks, determining the lifetime of the blocks in the cache. SRRIP [19] statically predicts an “intermediate” re‑reference interval at cache insertion time and updates the re‑reference prediction on subsequent accesses. DRRIP [19] improves performance by selecting the inserting position among different policies. More recent studies exploit long‑term information by analyzing evicted blocks. SHiP [48] and SHiP++ [53] provide finer‑granularity re‑reference prediction by correlating re‑reference behavior to PCs and learning the past behavior of SRRIP. SHiP uses a history table (SHCT) to learn the re‑reference characteristic for each signature and updates on hits and evictions. SHiP++ enhances predictions and SHCT training. Hawkeye [17] learns Belady’s optimal solution over a long history and formulates re‑reference prediction as a binary problem (“cache‑friendly” vs. “cache‑averse”). Mockingjay [41] mimics Belady’s optimal solution and introduces a multi‑class re‑reference policy.

**Machine learning for re‑reference prediction.** Perceptron learning has been used for re‑reference prediction [45]. Glider [42] trains an offline attention‑based LSTM, then distills to an Integer SVM. Reinforcement learning has also been explored to derive cost‑effective policies (RLR) [40]. However, training overhead, computation cost, and model size make many ML approaches hard to justify on latency‑critical caches. Our design emphasizes efficiency and lightweight implementation without preprocessing overhead.

### D. Cost‑based Cache Management

Unlike locality‑based schemes that focus on reducing misses, several works improve performance by selectively eliminating **expensive** misses. LACS [22] estimates miss cost from the number of instructions issued during an LLC miss, but it is not cycle‑accurate. SBAR [34] is MLP‑aware and reduces costly **isolated** misses, but it does not consider **hit‑miss** overlapping (Section II‑A), leaving room for improvement.

---

## III. MOTIVATION

### A. The Limitations of Locality‑based Cache Management

With many cores and concurrent applications, a shared LLC observes mixed access patterns. Mixed patterns degrade the effectiveness of purely locality‑based schemes, which depend on specific access patterns. At the LLC, recency‑friendly patterns are filtered by upper‑level caches, further reducing direct benefits [48], [49]. Locality‑based schemes also optimize a single goal—reducing misses—which works for sequential access but is less suitable for prevalent concurrent cache/memory activity.

### B. The Limitations of MLP‑based Cache Management

Not all cache misses have the same impact in a system with access concurrency [30], [34]. Eliminating isolated (high‑cost) misses helps more than eliminating concurrent (low‑cost) misses; SBAR [34] follows this idea. However, MLP‑based cost ignores **hit‑miss** overlap.

A study case (Figure 2 in the paper) shows six accesses (B, F are hits; A, C, D, E are misses). Each access has 2 base cycles; each miss has 6 additional miss cycles. Using the MLP definition: access A has the highest MLP‑based cost (5) because cycles 3–6 are not overlapped by other misses, whereas C, D, and E each compute to 7/3 due to shared overlapping among misses (details equivalent to Table I in the paper).

Re‑evaluated with the C‑AMAT view, only C, D, and E are **pure misses**. A’s miss cycles are fully overlapped by base cycles of other accesses; hence A does not hurt performance most. C, D, and E have different **pure miss** spans, so their contributions to performance loss differ—even though their MLP costs match—because MLP ignores hit‑miss overlap.

Across 4‑core multi‑copy workloads (LRU policy), **30%–80%** of LLC misses exhibit hit‑miss overlap from the same core (Figure 3), which must be accounted for. LLC pure misses increase latency to upper‑level caches, degrading performance. We therefore target eliminating **costly pure misses** at the LLC.

---

## IV. PURE MISS CONTRIBUTION

### A. Definition

**Pure Miss Contribution (PMC)** is a new metric that integrates locality, concurrency, and overlapping. PMC recognizes that not all outstanding misses have the same cost and identifies high‑cost misses to optimize performance. PMC is defined as a miss’s contribution to the **total active pure miss cycles** from the same core. A miss with at least one pure miss cycle has **PMC > 0**. If all miss cycles are overlapped by base cycles, **PMC = 0**.

### B. Measurement and Implementation

Algorithm 1 (in the paper) measures PMC per cache level (l). A per‑core bit **NoNewAccess(_x)** indicates whether any base access from core (x) is present in the current cycle at level (l). If set, there are no base accesses to hide miss cycles, so any outstanding misses from core (x) are in **active pure miss** state.

Modern systems track outstanding misses with **MSHRs** [46]. We add a **PMC** field to each MSHR entry. When a miss allocates an MSHR entry, its PMC counter is initialized to zero. During **active pure miss** cycles for core (x), each outstanding miss’s PMC is incremented by (1/N_x), where (N_x) is the number of outstanding misses from that core at level (l). This evenly apportions each pure miss cycle among concurrent pure misses. When the miss is served, the PMC is optionally quantized and stored with the block’s tag‑store metadata to guide policies (Section V‑F).

A practical **PMC Measurement Logic (PML)** comprises an **Access Detector (AD)** to detect base access windows and drive **NoNewAccess**, a **Pure Miss Detector (PMD)** that uses AD + MSHR state to identify pure misses, and a **PMC Calculation Unit (PCU)** that updates PMC each cycle. The PCU uses a small lookup table of reciprocals (1/N) (with (N\le 64) for typical MSHR sizes) to avoid a divider.

**Parallel multi‑threading.** PML instances are per‑core. If a core runs multiple threads, any thread’s memory activity contributes to that core’s active cycles; PMC remains defined per core.

### C. Revisit the Study Case with PMC Analysis

Using PMC: A has **PMC = 0** (entirely hidden). C has three pure miss cycles (10–12) overlapping with D and E, yielding **PMC = 1**. D and E each have five pure miss cycles (10–14) with overlaps, yielding **PMC = 2** apiece. The sum of PMCs equals the number of active pure miss cycles (5), aligning with the model. PMC captures hit‑miss overlap that MLP misses.

### D. Distribution and Predictability of PMC

Across 16 SPEC workloads (single‑core, LRU at LLC), PMC distributions show many misses with small PMC but non‑trivial tails (Figure 5). Define ( \text{PMC}*\delta ) as the absolute difference between consecutive PMCs for misses from the same PC. For all workloads, most ( \text{PMC}*\delta ) values are < 50 cycles, and the median per workload is small (Table III), indicating **high predictability per PC**—which we exploit in CARE (Section V).

---

## V. CARE: CONCURRENCY‑AWARE CACHE MANAGEMENT

### A. Overview

**CARE** is an LLC management framework combining locality and concurrency awareness. It associates each access with a **PC‑based signature** and learns both **re‑reference** and **PMC** behavior. A **Signature History Table (SHT)** stores **Re‑reference Confidence (RC)** and **PMC Degree (PD)** per signature. A **Signature‑Based Predictor (SBP)** predicts both reuse and cost to guide insertion and hit‑promotion policies. The PML provides PMCs, which are **quantized** via a **Dynamic Threshold Reconfiguration Mechanism (DTRM)** into 2‑bit **PMCS** states stored in block metadata, alongside a 1‑bit **R** (re‑referenced) and a 1‑bit **prefetch** tag, plus a 2‑bit **EPV** (eviction priority).

### B. Store and Update Access History in SHT

**Metadata.** On fill, **R=0**. PMC is quantized to **PMCS** using thresholds **PMC(_\text{low})** and **PMC(_\text{high})**:

* PMC < PMC(_\text{low}) → PMCS = 0
* PMC(*\text{low}) ≤ PMC ≤ PMC(*\text{high}) → PMCS = 1
* PMC > PMC(_\text{high}) → PMCS = 3

**SHT entries.** 16K entries, each with 3‑bit **RC** and 3‑bit **PD**. RC captures reuse tendency; PD captures propensity for high PMC.

**Updates.**

* **On hit:** set **R=1**. On the first re‑reference, increment RC (saturating) for that signature [53].
* **On eviction:** if **R=0**, decrement RC (saturating) [48], [53]. If **PMCS=0**, decrement PD; if **PMCS=3**, increment PD (both saturating).

### C. Predict Access Behavior with SBP

SBP classifies **High‑Reuse** if RC is saturated high, **Low‑Reuse** if RC=0, else **Moderate‑Reuse**. For cost, **High‑Cost** if PD saturated high; **Low‑Cost** if PD=0.

### D. CARE Policies (Insertion, Hit Promotion, Victim Selection)

Each block has a 2‑bit **EPV** (0 = most protected; 3 = evictable).

* **Insertion:**

  * High‑Reuse → EPV = 0
  * Low‑Reuse → EPV = 3
  * Moderate‑Reuse → use cost: Low‑Cost → EPV = 3; High‑Cost → EPV = 0; otherwise EPV = 2
* **Hit promotion:** High‑/Moderate‑Reuse → EPV = 0 on hit; Low‑Reuse → decrement EPV by 1 if EPV>0.
* **Victim selection:** choose any block with **EPV = 3**; if none, increment all EPVs and retry. **Writebacks** are inserted with EPV = 3 and are not promoted.

### E. Collaboration with Prefetching

Define pure misses identically for demand and prefetch requests; compute PMC as usual. Distinguish prefetches by adding a **prefetch** bit to the signature [53] so SHT learns separate behaviors. On a demand hit to a prefetched block, set **EPV = 3** (treat as expendable after its use). Promote to **EPV = 0** only if further useful accesses occur; ignore hits that are only prefetch re‑references.

### F. Dynamic Threshold Reconfiguration Mechanism (DTRM)

Quantize PMC to PMCS with adaptive thresholds. Initialize **PMC(_\text{high})=350** cycles and **PMC(_\text{low})=50** cycles. Count **TCM** = number of “costly” misses (PMC>PMC(_\text{high})) in each period (16K misses).

* If **TCM < 0.5% × 16K**, decrease PMC(*\text{low}) by 10 and PMC(*\text{high}) by 70.
* If **TCM > 5% × 16K**, increase PMC(*\text{low}) by 10 and PMC(*\text{high}) by 70.
  Update thresholds and use them for the next period.

### G. Hardware Cost and Complexity

Example: 2MB LLC, 64B lines. PML needs one **NoNewAccess** bit per core and a small reciprocal lookup (0.25KB with 64‑entry MSHR). Each MSHR stores a 32‑bit PMC. Per‑block metadata: 2‑bit **EPV**, 1‑bit **prefetch**. For SHT learning, use **set sampling** [33], [48]; in 64 sampled sets of a 16‑way cache, store per‑block **signature (14b)**, **R (1b)**, **PMCS (2b)**. SHT has 16K entries with 3‑bit **RC** and 3‑bit **PD**.
Total overhead ≈ **26.64KB** (≈1.3% of a 2MB LLC), including **6.76KB** specifically for concurrency awareness. Multi‑core adds a per‑core **NoNewAccess** bit and a small **core tag** in sampled‑set blocks.

---

## VI. METHODOLOGY

**Simulator.** ChampSim [4] (version used for IPC‑1 [1], also DPC‑3 [3], CRC‑2 [2]). Representative config (Table VII in the paper): 1–16 cores at 4GHz; private 32KB L1 I/D (8‑way, 4‑cycle, 8‑MSHR), private 256KB L2 (8‑way, 10‑cycle, 32‑MSHR), shared L3 = 2MB per core (16‑way, 20‑cycle, 64‑MSHR); L1 next‑line prefetcher, L2 IP‑stride; DRAM 2400 MT/s. LLC scales with core count. Warmup 50M instructions, then 200M simulated.

**Workloads.** SPEC CPU2006 [43] and CPU2017 [6] memory‑intensive traces (≥1 MPKI without prefetching), 30 total; traces from DPC‑3 via SimPoint [32]. GAP [9] workloads: bc, bfs, cc, pr, sssp; datasets orkut, twitter, urand; traces via Intel Pin [5]. Evaluate multi‑copy and mixed workloads (100 mixes); if a benchmark finishes early in a mix, it is replayed to reach 200M instructions per benchmark.

**Compared schemes.** LRU baseline; SHiP++ [53], Hawkeye [17], Glider [42], Mockingjay [41], and **M‑CARE** (our SBAR‑style variant that uses MLP‑based cost instead of PMC).

---

## VII. RESULTS

### A. CARE Performance Evaluation (4‑core, with prefetchers)

**SPEC multi‑copy.** CARE achieves **+10.3%** geometric‑mean speedup over LRU; SHiP++ +7.6%, Hawkeye +6.2%, Glider +7.2%, M‑CARE +7.5%. CARE reduces **LLC pMR** to **0.50** (tied best with Glider) and yields the **lowest average PMC** per miss (**95.11 cycles**) vs. 97.80 (M‑CARE), 97.98 (SHiP++), 99.44 (Hawkeye), 101.43 (Glider).

**GAP multi‑copy.** CARE outperforms LRU by **+8.7%**; SHiP++ +5.4%, Hawkeye +1.8%, Glider +3.0%, M‑CARE +6.7%. CARE’s advantage stems from concurrency awareness and a conservative hit policy that protects useful blocks.

**Mixed workloads (100).** CARE provides **+12.8%** geometric‑mean weighted speedup over LRU; SHiP++ +11.9%, Hawkeye +6.8%, Glider +6.4%, M‑CARE +11.4%. CARE is top on 67 mixes and is more stable across mixes.

### B. Scalability

**With prefetchers.** As core count rises (4→8→16), CARE’s improvement over LRU grows: **+10.3% → +13.0% → +17.1%** for SPEC multi‑copy; for GAP, CARE outperforms LRU, SHiP++, Hawkeye, Glider, Mockingjay, M‑CARE by larger margins at 16 cores. **AOCPA** (Average Overlapping Cycles Per Access) increases with core count (≈260.9→413.0→674.9), creating more opportunity for PMC‑guided decisions.

**Without prefetchers.** CARE continues to scale: in 16‑core SPEC multi‑copy, **+19.4%** over LRU (next best ≈ +11.9%, Mockingjay); in GAP multi‑copy, **+13.0%** over LRU.

---

## VIII. CONCLUSIONS

We emphasize the importance of data concurrency to memory performance and propose **PMC**, a comprehensive metric for weighing the performance cost of each cache miss. We develop a practical measurement mechanism for PMC and build **CARE**, a locality‑ and concurrency‑aware lightweight cache management framework. CARE integrates locality, concurrency, and overlapping to guide replacement and outperforms state‑of‑the‑art schemes across SPEC and GAP workloads, especially as concurrency increases.

---

## ACKNOWLEDGMENT

We thank the anonymous reviewers for their helpful feedback. This research is supported in part by the National Science Foundation under Grants CCF‑2029014, CCF‑2008907, CNS‑2152497, and by the NSF‑supported Chameleon testbed facility.

---

## REFERENCES

[1] “1st instruction prefetching championship.” [https://research.ece.ncsu.edu/ipc/](https://research.ece.ncsu.edu/ipc/).
[2] “2nd cache replacement championship.” [https://crc2.ece.tamu.edu/](https://crc2.ece.tamu.edu/).
[3] “3rd data prefetching championship.” [https://dpc3.compas.cs.stonybrook.edu/?final_programs](https://dpc3.compas.cs.stonybrook.edu/?final_programs).
[4] “The ChampSim simulator,” [https://github.com/ChampSim/ChampSim](https://github.com/ChampSim/ChampSim).
[5] “Pin—a dynamic binary instrumentation tool,” [https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html](https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html).
[6] “SPEC CPU2017 benchmark suite,” [http://www.spec.org/cpu2017/](http://www.spec.org/cpu2017/).
[7] A. Agarwal, K. Roy, and T. Vijaykumar, “Exploring high bandwidth pipelined cache architecture for scaled technology,” in *DATE*, 2003.
[8] V. Balaji, N. Crago, A. Jaleel, and B. Lucia, “P‑opt: Practical optimal cache replacement for graph analytics,” in *HPCA*, 2021.
[9] S. Beamer, K. Asanović, and D. Patterson, “The GAP benchmark suite,” *arXiv:1508.03619*, 2015.
[10] L. A. Belady, “A study of replacement algorithms for a virtual‑storage computer,” *IBM Systems Journal*, 1966.
[11] M. Chaudhuri, “Pseudo‑LIFO: The foundation of a new family of replacement policies for last‑level caches,” in *MICRO*, 2009.
[12] Y. Chou, B. Fahs, and S. Abraham, “Microarchitecture optimizations for exploiting memory‑level parallelism,” in *ISCA*, 2004.
[13] N. Duong et al., “Improving cache management policies using dynamic reuse distances,” in *MICRO*, 2012.
[14] P. Faldu, J. Diamond, and B. Grot, “Domain‑specialized cache management for graph analytics,” in *HPCA*, 2020.
[15] H. Gao and C. Wilkerson, “A dueling segmented LRU replacement algorithm with adaptive bypassing,” in *JWAC (CRC)*, 2010.
[16] A. Glew, “MLP yes! ILP no,” *ASPLOS Wild and Crazy Ideas*, 1998.
[17] A. Jain and C. Lin, “Back to the future: Leveraging Belady’s algorithm for improved cache replacement,” in *ISCA*, 2016.
[18] A. Jain and C. Lin, “Rethinking Belady’s algorithm to accommodate prefetching,” in *ISCA*, 2018.
[19] A. Jaleel et al., “High performance cache replacement using re‑reference interval prediction (RRIP),” *SIGARCH News*, 2010.
[20] D. A. Jiménez and E. Teran, “Multiperspective reuse prediction,” in *MICRO*, 2017.
[21] G. Keramidas, P. Petoumenos, and S. Kaxiras, “Cache replacement based on reuse‑distance prediction,” in *ICCD*, 2007.
[22] M. Kharbutli and R. Sheikh, “LACS: A locality‑aware cost‑sensitive cache replacement algorithm,” *IEEE TC*, 2013.
[23] M. Kharbutli and Y. Solihin, “Counter‑based cache replacement and bypassing algorithms,” *IEEE TC*, 2008.
[24] D. Kroft, “Lockup‑free instruction fetch/prefetch cache organization,” in *25 Years of ISCA*, 1998.
[25] A.‑C. Lai, C. Fide, and B. Falsafi, “Dead‑block prediction & dead‑block correlating prefetchers,” in *ISCA*, 2001.
[26] D. Lee et al., “LRFU: A spectrum of policies that subsumes LRU and LFU,” *IEEE TC*, 2001.
[27] H. Liu et al., “Cache bursts: Eliminating dead blocks and increasing cache efficiency,” in *MICRO*, 2008.
[28] J. Liu, P. Espina, and X.‑H. Sun, “A study on modeling and optimization of memory systems,” *JCST*, 2021.
[29] Y. Liu and X.‑H. Sun, “LPM: A methodology for concurrent data access pattern optimization,” *TPDS*, 2019.
[30] X. Lu, R. Wang, and X.‑H. Sun, “APAC: An accurate and adaptive prefetch framework with concurrent memory access analysis,” in *ICCD*, 2020.
[31] X. Lu, R. Wang, and X.‑H. Sun, “PREMIER: A concurrency‑aware pseudo‑partitioning framework for shared LLC,” in *ICCD*, 2021.
[32] E. Perelman et al., “Using SimPoint for accurate and efficient simulation,” *SIGMETRICS PER*, 2003.
[33] M. K. Qureshi et al., “Adaptive insertion policies for high performance caching,” *SIGARCH News*, 2007.
[34] M. K. Qureshi et al., “A case for MLP‑aware cache replacement,” in *ISCA*, 2006.
[35] M. K. Qureshi, D. Thompson, and Y. N. Patt, “The V‑way cache: Demand‑based associativity via global replacement,” in *ISCA*, 2005.
[36] K. Rajan and G. Ramaswamy, “Emulating optimal replacement with a shepherd cache,” in *MICRO*, 2007.
[37] J. A. Rivers et al., “On high‑bandwidth data cache design for multi‑issue processors,” in *MICRO*, 1997.
[38] J. T. Robinson and M. V. Devarakonda, “Frequency‑based replacement,” in *SIGMETRICS*, 1990.
[39] V. Seshadri et al., “The evicted‑address filter,” in *PACT*, 2012.
[40] S. Sethumurugan, J. Yin, and J. Sartori, “Designing a cost‑effective cache replacement policy using machine learning,” in *MICRO*, 2011.
[41] I. Shah, A. Jain, and C. Lin, “Effective mimicry of Belady’s MIN policy,” in *HPCA*, 2022.
[42] Z. Shi, X. Huang, A. Jain, and C. Lin, “Applying deep learning to the cache replacement problem,” in *MICRO*, 2019.
[43] C. D. Spradling, “SPEC CPU2006 benchmark tools,” *SIGARCH News*, 2007.
[44] X.‑H. Sun and D. Wang, “Concurrent average memory access time,” *Computer*, 2013.
[45] E. Teran, Z. Wang, and D. A. Jiménez, “Perceptron learning for reuse prediction,” in *MICRO*, 2016.
[46] J. Tuck, L. Ceze, and J. Torrellas, “Scalable cache miss handling for high MLP,” in *MICRO*, 2006.
[47] W. A. Wong and J.‑L. Baer, “Modified LRU policies for improving second‑level cache behavior,” in *HPCA‑6*, 2000.
[48] C.‑J. Wu et al., “SHiP: Signature‑based hit predictor for high performance caching,” in *MICRO*, 2011.
[49] C.‑J. Wu et al., “PACMan: Prefetch‑aware cache management for high performance caching,” in *HPCA*, 2021.
[50] W. A. Wulf and S. A. McKee, “Hitting the memory wall: Implications of the obvious,” *SIGARCH News*, 1995.
[51] Y. Xie and G. H. Loh, “PIPP: Promotion/insertion pseudo‑partitioning of multi‑core shared caches,” *SIGARCH News*, 2009.
[52] L. Yan et al., “COPIM: A concurrency‑aware PIM offloading architecture for graph applications,” in *ISLPED*, 2021.
[53] V. Young et al., “SHiP++: Enhancing signature‑based hit predictor for improved cache performance,” *CRC’17*, 2017.
